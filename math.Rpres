Bayesian Regression Models with RStanARM
========================================================
author: Tristan Mahr
date: Sept. 21, 2016
autosize: true
incremental: true

```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.asp = 0.618,
  fig.width = 6,
  dpi = 300,
  fig.align = "center",
  out.width = "70%"
)
library("dplyr", warn.conflicts = FALSE)
library("ggplot2")
```


Some math
========================================================
type: section


The Slide with the Theorem on it
========================================================

I saw a creature, and it just _quacked_ at me! Was it a duck?

$$ P(\mathrm{duck} \mid \mathrm{quacks}) = \frac{ P(\mathrm{quacks} \mid \mathrm{duck}) \, P(\mathrm{duck})}{P(\mathrm{quacks})} $$

* If it quacks like a duck...
* and after taking into account how common ducks are and how often animals quack...
* then it probably is a duck.


Classifying emails
========================================================

$$ P(\mathrm{spam} \mid \mathrm{cialis}) = \frac{ P(\mathrm{cialis} \mid \mathrm{spam}) \, P(\mathrm{spam})}{P(\mathrm{cialis})} $$

The two unconditional probabilities are base rates that need to be accounted for.

$$ P(\mathrm{spam} \mid \mathrm{cialis}) = \frac{\mathrm{cialis\ freq.\ in\ spam} \, * \mathrm{spam\ rate}}{\mathrm{cialis\ freq.}} $$



More generally
========================================================

How plausible is some hypothesis given the data?

$$ P(\mathrm{hypothesis} \mid \mathrm{data}) = \frac{ P(\mathrm{data} \mid \mathrm{hypothesis}) \, P(\mathrm{hypothesis})}{P(\mathrm{data})} $$

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$



Likelihood is fit
========================================================

We found some IQ scores in an old, questionable dataset.

```{r,  width = 35}
library("dplyr")
iqs <- car::Burt$IQbio
iqs
```

By design, IQs are scaled to have a population mean of 100 and an SD of 15.

How well do these data *fit* that kind of bell curve?


Density is a measure of relative likelihood
========================================================
left: 40%

* Draw the data on a hypothetical bell curve with a mean of 100 and SD of 15.
* Height of each point on the curve is the density around that point.
* Higher density regions are more likely.
* Data farther from center is less likely.

```{r, echo = FALSE}
library("ggplot2")
iq_df <- function(mean, sd = 15) {
  data_frame(
    iq = seq(min(iqs), max(iqs), by = 1),
    density = dnorm(iq, mean, 15),
    mean = mean,
    sd = sd)
}
```

***

```{r, echo = FALSE}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line() +
  geom_point(aes(x = iqs, y = dnorm(iqs, 100, 15)), data = data_frame(iqs)) +
  geom_segment(aes(x = iqs, xend = iqs, y = 0, yend = dnorm(iqs, 100, 15)), data = data_frame(iqs))
p
```





Likelihood
========================================================
title: false

Density function `dnorm(xs, mean = 100, sd = 15)` tells us the height of each
value in `xs` when drawn on a normal bell curve

```{r}
# likelihood (density) of each point
dnorm(iqs, 100, 15) %>% round(3)

# likelihood of all points is their product
prod(dnorm(iqs, 100, 15))

# but quantities gets vanishingly small so we use log scale
sum(dnorm(iqs, 100, 15, log = TRUE))
```




Likelihood
========================================================
title: false

Which mean best fits the data? Below average IQ (85), average IQ (100), or above average IQ (115)?

```{r, collapse = TRUE, comment = "#>"}
sum(dnorm(iqs, 85, 15, log = TRUE))
sum(dnorm(iqs, 100, 15, log = TRUE))
sum(dnorm(iqs, 115, 15, log = TRUE))
```

* The data fit best wihh the "average" average mean.
* We just used a "maximum likelihood" criterion to choose among these alternatives!


Likelihood
=============================

- Likelihood is how well data fit in a distribution.
- Classical regression's "line of best fit" estimates the parameters that maximize the likelihood of the data.




The Slide With The Theorem On It
========================================================
title: false

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$

Update our prior information in proportion to how well the data fits with that information.

Bayesian updating animation?


























Basic regression model
========================================================

We want to estimate a response $y$ with 1 predictor $x_1$.

$$
\begin{align*}
   y_i &\sim \mathrm{Normal}(\mathrm{mean} = \mu_i, \mathrm{SD} = \sigma)
   \\
  \mu_i &= \alpha + \beta_1*x_{1i}
\end{align*}
$$

Observation $y_i$ is a draw from a normal distribution centered around a mean $\mu_i$ with a standard deviation of $\sigma$.

We estimate the mean with a constant "intercept" term $\alpha$ plus a linear combination of predictor variables (just $x_1$ for now).



Basically
=============================================================================

```{r, echo = FALSE}
library("ggplot2")
library("dplyr")

# Some toy data
davis <- car::Davis %>%
  filter(100 < height)

# Fit a model and estimate mean at five points
m <- lm(weight ~ height, davis)
newdata <- data_frame(height = c(15:19 * 10))
newdata$fit <- predict(m, newdata)

# get density of random normal values
get_density_df <- function(mean, sd, steps) {
  ends <- qnorm(c(.001, .999), mean, sd)
  steps <- seq(ends[1], ends[2], length.out = steps)

  df <- data_frame(
    value = steps,
    density = dnorm(steps, mean, sd))
  df
}

# Get a distribution at each mean
simulated <- newdata %>%
  group_by(height) %>%
  do(get_density_df(.$fit, sigma(m), 10000)) %>%
  ungroup


ggplot(simulated) +
  # Plot at each mean, adding some scaled value of density to the mean.
  aes(x = height - (100 * density), y = value, group = height) +
  geom_polygon(fill = "grey50") +
  # raw data
  geom_point(aes(height, weight), data = davis) +
  labs(x = "height", y = "weight")

```

Bayesian stats
========================================================

Parameters we need to estimate for regression: $\alpha, \beta_1, \sigma$

- A classical model provides one model of many plausible models of the data.
  It'll find the parameters that maximize likelihood.
- A Bayesian model is a model of models. We get a *distribution of models* that
  are consistent with the data.




This is where things get difficult
========================================================

Parameters we need to estimate: $\alpha, \beta_1, \sigma$

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$

$$ P(\alpha, \beta, \sigma \mid x) = \frac{ P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma)}{\iiint \, P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma) \,d\alpha \,d\beta \,d\sigma} $$

Things get gnarly. We don't perform this integral calculus.

This is the black-box step. Instead, we rely on Markov-chain Monte Carlo simulation to get us samples from the posterior. Those samples will provide a detailed picture of the posterior.

Enter Stan.

