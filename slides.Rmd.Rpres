Bayesian Regression Models with RStanARM
========================================================
author: Tristan Mahr
date: Sept. 21, 2016
autosize: true
incremental: true


<!-- How I Got into Bayesian Statistics -->
<!-- ======================================================= -->

<!-- - I did all of my statistics coursework through psychology and educational psychology. -->
<!-- - And psychology -->


Learning More
===============================================================================


You are trained in classical regression
===============================================================================

You want to read [_Statistical Rethinking_]() and [watch the lectures](https://www.youtube.com/playlist?list=PLDcUM9US4XdMdZOhJWJJD4mDBMnbTWw_z).

![](./assets/unlearn.gif)

***

![](./assets/rethinking.jpg)



'Rethinking'
===============================================================================

* This book is exceptional, stuffed to the brim with trivia, advice, and wisdom.
* Not just a book about statistics, but how we use statistical models in scientific practice.
* Explains side issues like MCMC, basics of information theory, or why the normal distribution is so prevalent -- but just deep enough for the reader to get the intuitions needed for practice.
* My sole criticism is that its companion R `rethinking` package is not on CRAN.


Rethinking 1
===============================================================================
title:false

![](./assets/vampire.PNG)

Rethinking 2
===============================================================================
title:false

![](./assets/meet-the-family.PNG)

***

![](./assets/conditioning.PNG)


Rethinking 3
===============================================================================
title:false











Overview
========================================================

- How I got into Bayesian statistics
- Some intuition-building about Bayes theorem
- Tour of RStanARM
- Where to learn more about Bayesian statistics




The "Crisis" in Psychology
========================================================
left: 50%

[Open Science Collaboration (2015)](http://science.sciencemag.org/content/349/6251/aac4716) tries to replicate 100 studies published in 3 psychology different journals in 2008.

![Scatter plot of original vs replicated effect sizes](./assets/reproducibility.PNG)

***

- Boil a study down to 1 effect size and 1 test statistic.
  - Compare replication's test statistic and magnitude of effect size against original.
- Approximately 36% of the studies are replicated.
- On average, the effect size in the replication is half that of the original study.

Most studies did not replicate, and the replicated effects were considerably
smaller than the original effects.



Reactions
========================================================

We're doomed.

![](./assets/throne-of-lies.gif)

Most findings are probably false.

***

No, this is business as usual.

![](./assets/business-as-usual.gif)

Any credible discipline has to do this kind of house-cleaning from time to time.


It's still going...
========================================================

Some famous findings are failing to replicate.

- [Ego depletion](https://www.youtube.com/watch?v=2MDNvKXdLEM)
- [Facial feedback](http://www.slate.com/articles/health_and_science/cover_story/2016/08/can_smiling_make_you_happier_maybe_maybe_not_we_have_no_idea.html)
- [Power pose](http://www.slate.com/articles/health_and_science/science/2016/01/amy_cuddy_s_power_pose_research_is_the_latest_example_of_scientific_overreach.html)
- [Others](http://nymag.com/scienceofus/2016/09/a-helpful-rundown-of-psychologys-replication-crisis.html?mid=twitter-share-scienceofus)


****

![](./assets/this-is-fine.jpeg) <small>&mdash; @DrPrimestein <a href="https://twitter.com/DrPrimestein/status/770210303963463681">August 29, 2016</a></small>


Lots of hand-wringing and soul-searching
=============================================

Some reactionary:

- Replication creates an [industry for incompetent hacks](http://www.sciencedirect.com/science/article/pii/S002210311600007X).
- Oh, great here come the [replication bullies](http://science.sciencemag.org/content/344/6186/788.summary).
- And the [methodological terrorists](https://twitter.com/maltoesermalte/status/777999804911812609).

![](./assets/dumpster-fire.gif)

***

Some constructive:

- What does a failed replication even mean?
- [Everything is f'ed](https://hardsci.wordpress.com/2016/08/11/everything-is-fucked-the-syllabus/) -- so what else is new?




I'm optimistic
========================================================

Better research practices are catching on.

- Pre-registration
- Power analyses
- Meta-analytic tools to assess the health of a field
- Better disclosure of other unanalyzed measurements
- Etc.




The crisis made me more aware of questionable research practices
========================================================

- HARKing (hypothesizing after results are known)
  - Telling a story to fit the data.
- Garden of forking data
  - Conducting countless sub-tests and sub-analyses on the data
- p-hacking
  - Doing these tests in order to find a significant effect.
- Selective reporting
  - Reporting only the tests that yielded a significant result.
- And so on.




My response
========================================================

- Psychology's soul-searching about the replication crisis made me want to level up my stats.
- In particular, I wanted something besides statistical significance.
  - p-values don't mean what you probably think they mean.
  - Neither do confidence intervals.
  - Statistical significance is not related to practical significance.




ARM
========================================================

- December, 2015. Started reading ARM book.
- It emphasized estimation, uncertainty and simulation.
- Midway through, the book pivots to Bayesian estimation.
- I roll with it.






January 2016
========================================================

There was an influx of Bayesian tools for R around January.

- I'm down the rabbit hole, writing Stan models to fit the models from the ARM book.
- [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/), a book that reteaches regression from a Bayesian perspective with R and Stan, is released (12/2015).
- A new version of [brms](https://cran.r-project.org/web/packages/brms/index.html) is released (1/2016). This package converts R model code into Stan programs.
- [RStanARM](https://cran.r-project.org/web/packages/rstanarm/index.html) is released (1/2016).

https://thinkinator.com/2016/01/12/r-users-will-now-inevitably-become-bayesians/


I eat all this up.


That's how
===========================================================
That's how I became a Bayesian.















Basic regression model
========================================================

We want to estimate a response $y$ with 2 predictors $x_1, x_2$.

$$
\begin{align*}
   y_i &\sim \mathrm{Normal}(\mathrm{mean} = \mu_i, \mathrm{SD} = \sigma)
   \\
  \mu_i &= \alpha + \beta_1*x_{1i} + \beta_{2}*x_{2i}
\end{align*}
$$

Observation $y_i$ is a draw from a normal distribution centered around a mean $\mu_i$ with a standard deviation of $\sigma$.

We estimate the mean with a constant "intercept" term $\alpha$ plus a linear combination of predictor variables ($x_1, x_2$).


Parameters we need to estimate
========================================================

$\alpha, \beta_1, \beta_2$










Back to Bayes
=============================

My prior is that any value between 70-130 is equally likely to be the mean of the IQs, and that the SD of the IQs is fixed at 15.


```{r}




df1$likelihood

df %>%
  mutate()

priors <- expand.grid(mean = p_mean, sd = p_sd) %>%
  mutate(prior = 1) %>%
  as_data_frame()

priors

prior <- rep(1 , 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
# Sampling from the posterior
samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)

plot(samples)


possible_means <- i

iqs
```














***






Values with higher density are more likely in that bell curve.


Draw a normal bell-curve with a mean of 100 and SD of 15.


```{r}
 geom_density(aes(x)) +
  geom_point(aes(iqs, y = dnorm(iqs, 100, 15)), data = data_frame(x = iqs))

```




Draw observations on a normal bell-curve (100, 15). Height of the point on the bell curve.

```{r}
ggplot()
```



```{r}
pop_average <- dnorm(iqs, mean = 100, sd = 15)
pop_average %>% round(3)
mean(pop_average)

below_average <- dnorm(iqs, mean = 85, sd = 15)
above_average <- dnorm(iqs, mean = 115, sd = 15)

mean(below_average)
mean(above_average)

ggplot(data_frame(iqs), aes(x = iqs)) +
  geom_point(aes(y = pop_average, color = "100")) +
  geom_point(aes(y = below_average, color = "85")) +
  geom_point(aes(y = above_average, color = "115"))


```














The line of best fit from classical regression is that maximizes likelihood.

```{r}



library("ggplot2")
library("dplyr")
ggplot(car::Burt) +
  aes(x = IQbio) +
  geom_line(aes(y = dnorm(IQbio, 70, 15), color = "70, 15")) +
  geom_point(aes(y = dnorm(IQbio, 70, 15), color = "70, 15")) +
  geom_line(aes(y = dnorm(IQbio, 100, 15), color = "100, 15")) +
  geom_point(aes(y = dnorm(IQbio, 100, 15), color = "100, 15"))

geom_point(aes(y = density(IQbio)))

density(car::Burt$IQbio)[["y"]]
  geom_density(aes(x = IQbio, color = "raw data")) +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, mean(IQbio), sd(IQbio)), color = "cheating")) +
  +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, 85, 15), color = "85, 15")) +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, 100, 15), color = "100, 15")) +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, 145, 15), color = "115, 15"))


dnorm(car::Burt$IQbio, 70, 15) %>% mean
dnorm(car::Burt$IQbio, 85, 15) %>% mean
dnorm(car::Burt$IQbio, 100, 15) %>% mean
dnorm(car::Burt$IQbio, 115, 15) %>% mean

```




This is where things get difficult
========================================================

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$

$$ P(\alpha, \beta, \sigma \mid x) = \frac{ P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma)}{\iiint \, P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma) \,d\alpha \,d\beta \,d\sigma} $$

Things get gnarly. We don't do this integral calculus.

This is the black-box step. Instead, we rely on Markov-chain Monte Carlo simulation to get us samples from the posterior. Those samples will provide a detailed picture of the posters.

Enter Stan.



Bayesian stats
========================================================

- A frequentist model provides one model of many plausible models of the data. This model has certain properties and optimizes a certain penalty.
- A Bayesian model is a model of models. We get a distribution of plausible models of the data.
- We can quantify our uncertainty about the role of predictor by asking questions about the distribution of that predictor's parameter values.
- Priors...? They get conditioned/trained/updated by the data. We'll get back to them.















Software ecosystem
========================================================



What is Stan?
========================================================

- A programming language for probablistic stats.
- Write out a program using a mathy syntax.
- The model is _compiled_ into an executable that does the sampling.
- Pystan and RStan are interfaces that you send/receive data from the program.
- Simple example with RStan
- But ugh, I don't want to learn a whole new programming language right now.


What is RStanArm?
========================================================

- Batteries included, precompiled versions of common regression models.
- `glm` -> `stan_glm`, `glmer` -> `stan_glmer`.
- Write your regular code. Add `stan_` to the front, and add a prior.








default model summary
========================================================


```{r}
model <- lm(Sepal.Length ~ Sepal.Width, iris)
summary(model)
```

arm: opinionated
========================================================

ARM just reports _B_ and SE.

```{r, message = FALSE}
library("dplyr")
arm::display(model)
```




arm: opinionated
========================================================


```{r}
model_sim <- arm::sim(model)

coef(model_sim) %>% apply(2, quantile, probs = c(.025, .25, .5, .75, .975))
```


- Psychology's soul-searching about replication crisis made me want to level up my stats.
- Started reading ARM book. It emphasized uncertainty and simulation.





  - Similar perspective: https://speakerdeck.com/jakevdp/statistics-for-hackers
  - arm package's opinionated interface. No p value.
  - Lazy posteriors: Use point estimate of model parameter and its standard error to seed a distribution. Draw from that distribution. Now you have an interval of plausible model parameters.
- Most NHST practitioners cannot correctly interpret confidence intervals or p-values.


- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3







The Slide With The Theorem On It
========================================================

I saw a creature, and it just _quacked_ at me! Was it a duck?

$$ P(\mathrm{duck} \mid \mathrm{quacks}) = \frac{ P(\mathrm{quacks} \mid \mathrm{duck}) \, P(\mathrm{duck})}{P(\mathrm{quacks})} $$

How plausible is some possibility given the data?

$$ P(\mathrm{hypothesis} \mid \mathrm{data}) = \frac{ P(\mathrm{data} \mid \mathrm{hypothesis}) \, P(\mathrm{hypothesis})}{P(\mathrm{data})} $$

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$



Likelihood is fit
========================================================

We found some IQ scores in an old, questionable dataset.

```{r,  width = 35}
library("dplyr")
iqs <- car::Burt$IQbio
iqs
```

IQs are scaled to have a population mean of 100 and an SD of 15.

How well do these data fit that kind of bell curve?


Density is a measure of likelihood
========================================================
left: 40%

* Draw the data on a bell curve on a hypothetical bell curve a mean of 100 and
  SD of 15.
* Height of each point on the curve is the density around that point.
* Higher density regions are more likely.
* Density decreases (data less likely) are data gets farther from mean.

```{r, echo = FALSE}
library("ggplot2")
iq_df <- function(mean, sd = 15) {
  data_frame(
    iq = seq(min(iqs), max(iqs), by = 1),
    density = dnorm(iq, mean, 15),
    mean = mean,
    sd = sd)
}
```

***

```{r, echo = FALSE}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line() +
  geom_point(aes(x = iqs, y = dnorm(iqs, 100, 15)), data = data_frame(iqs))
p
```





Likelihood
========================================================
title: false

Density function `dnorm(xs, mean = 100, sd = 15)` tells us the height of each
value in `xs` when drawn on a normal bell curve

```{r}
# likelihood (density) of each point
dnorm(iqs, 100, 15) %>% round(3)

# likelihood of all points is their product
prod(dnorm(iqs, 100, 15))

# but quantities gets vanishingly small so we use log scale
sum(dnorm(iqs, 100, 15, log = TRUE))
```




Likelihood
========================================================
title: false

Which mean fits the data the best? Below average 85, average 100, or above average 115?

```{r}
sum(dnorm(iqs, 85, 15, log = TRUE))
sum(dnorm(iqs, 100, 15, log = TRUE))
sum(dnorm(iqs, 115, 15, log = TRUE))
```

We just used a "maximum likelihood" criterion to choose among these alternatives!


Likelihood
=============================

- Likelihood is how well data fit in a distribution.
- Classical regression's "line of best fit" estimates the parameters that maximize the likelihood of the data.














