Bayesian Regression Models with RStanARM
========================================================
author: Tristan Mahr
date: Sept. 21, 2016
autosize: true
incremental: true


















Basic regression model
========================================================

We want to estimate a response $y$ with 2 predictors $x_1, x_2$.

$$
\begin{align*}
   y_i &\sim \mathrm{Normal}(\mathrm{mean} = \mu_i, \mathrm{SD} = \sigma)
   \\
  \mu_i &= \alpha + \beta_1*x_{1i} + \beta_{2}*x_{2i}
\end{align*}
$$

Observation $y_i$ is a draw from a normal distribution centered around a mean $\mu_i$ with a standard deviation of $\sigma$.

We estimate the mean with a constant "intercept" term $\alpha$ plus a linear combination of predictor variables ($x_1, x_2$).


Parameters we need to estimate
========================================================

$\alpha, \beta_1, \beta_2$










Back to Bayes
=============================

My prior is that any value between 70-130 is equally likely to be the mean of the IQs, and that the SD of the IQs is fixed at 15.


```{r}




df1$likelihood

df %>%
  mutate()

priors <- expand.grid(mean = p_mean, sd = p_sd) %>%
  mutate(prior = 1) %>%
  as_data_frame()

priors

prior <- rep(1 , 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
# Sampling from the posterior
samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)

plot(samples)


possible_means <- i

iqs
```














***






Values with higher density are more likely in that bell curve.


Draw a normal bell-curve with a mean of 100 and SD of 15.


```{r}
 geom_density(aes(x)) +
  geom_point(aes(iqs, y = dnorm(iqs, 100, 15)), data = data_frame(x = iqs))

```




Draw observations on a normal bell-curve (100, 15). Height of the point on the bell curve.

```{r}
ggplot()
```



```{r}
pop_average <- dnorm(iqs, mean = 100, sd = 15)
pop_average %>% round(3)
mean(pop_average)

below_average <- dnorm(iqs, mean = 85, sd = 15)
above_average <- dnorm(iqs, mean = 115, sd = 15)

mean(below_average)
mean(above_average)

ggplot(data_frame(iqs), aes(x = iqs)) +
  geom_point(aes(y = pop_average, color = "100")) +
  geom_point(aes(y = below_average, color = "85")) +
  geom_point(aes(y = above_average, color = "115"))


```














The line of best fit from classical regression is that maximizes likelihood.

```{r}



library("ggplot2")
library("dplyr")
ggplot(car::Burt) +
  aes(x = IQbio) +
  geom_line(aes(y = dnorm(IQbio, 70, 15), color = "70, 15")) +
  geom_point(aes(y = dnorm(IQbio, 70, 15), color = "70, 15")) +
  geom_line(aes(y = dnorm(IQbio, 100, 15), color = "100, 15")) +
  geom_point(aes(y = dnorm(IQbio, 100, 15), color = "100, 15"))

geom_point(aes(y = density(IQbio)))

density(car::Burt$IQbio)[["y"]]
  geom_density(aes(x = IQbio, color = "raw data")) +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, mean(IQbio), sd(IQbio)), color = "cheating")) +
  +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, 85, 15), color = "85, 15")) +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, 100, 15), color = "100, 15")) +
  geom_line(aes(x = IQbio, y = dnorm(IQbio, 145, 15), color = "115, 15"))


dnorm(car::Burt$IQbio, 70, 15) %>% mean
dnorm(car::Burt$IQbio, 85, 15) %>% mean
dnorm(car::Burt$IQbio, 100, 15) %>% mean
dnorm(car::Burt$IQbio, 115, 15) %>% mean

```




This is where things get difficult
========================================================

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$

$$ P(\alpha, \beta, \sigma \mid x) = \frac{ P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma)}{\iiint \, P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma) \,d\alpha \,d\beta \,d\sigma} $$

Things get gnarly. We don't do this integral calculus.

This is the black-box step. Instead, we rely on Markov-chain Monte Carlo simulation to get us samples from the posterior. Those samples will provide a detailed picture of the posters.

Enter Stan.



Bayesian stats
========================================================

- A frequentist model provides one model of many plausible models of the data. This model has certain properties and optimizes a certain penalty.
- A Bayesian model is a model of models. We get a distribution of plausible models of the data.
- We can quantify our uncertainty about the role of predictor by asking questions about the distribution of that predictor's parameter values.
- Priors...? They get conditioned/trained/updated by the data. We'll get back to them.





















default model summary
========================================================


```{r}
model <- lm(Sepal.Length ~ Sepal.Width, iris)
summary(model)
```

arm: opinionated
========================================================

ARM just reports _B_ and SE.

```{r, message = FALSE}
library("dplyr")
arm::display(model)
```




arm: opinionated
========================================================


```{r}
model_sim <- arm::sim(model)

coef(model_sim) %>% apply(2, quantile, probs = c(.025, .25, .5, .75, .975))
```


- Psychology's soul-searching about replication crisis made me want to level up my stats.
- Started reading ARM book. It emphasized uncertainty and simulation.





  - Similar perspective: https://speakerdeck.com/jakevdp/statistics-for-hackers
  - arm package's opinionated interface. No p value.
  - Lazy posteriors: Use point estimate of model parameter and its standard error to seed a distribution. Draw from that distribution. Now you have an interval of plausible model parameters.
- Most NHST practitioners cannot correctly interpret confidence intervals or p-values.


- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3


First Slide
========================================================

For more details on authoring R presentations please visit <https://support.rstudio.com/hc/en-us/articles/200486468>.

- Bullet 1
- Bullet 2
- Bullet 3







The Slide With The Theorem On It
========================================================

I saw a creature, and it just _quacked_ at me! Was it a duck?

$$ P(\mathrm{duck} \mid \mathrm{quacks}) = \frac{ P(\mathrm{quacks} \mid \mathrm{duck}) \, P(\mathrm{duck})}{P(\mathrm{quacks})} $$

How plausible is some possibility given the data?

$$ P(\mathrm{hypothesis} \mid \mathrm{data}) = \frac{ P(\mathrm{data} \mid \mathrm{hypothesis}) \, P(\mathrm{hypothesis})}{P(\mathrm{data})} $$

$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$



Likelihood is fit
========================================================

We found some IQ scores in an old, questionable dataset.

```{r,  width = 35}
library("dplyr")
iqs <- car::Burt$IQbio
iqs
```

IQs are scaled to have a population mean of 100 and an SD of 15.

How well do these data fit that kind of bell curve?


Density is a measure of likelihood
========================================================
left: 40%

* Draw the data on a bell curve on a hypothetical bell curve a mean of 100 and
  SD of 15.
* Height of each point on the curve is the density around that point.
* Higher density regions are more likely.
* Density decreases (data less likely) are data gets farther from mean.

```{r, echo = FALSE}
library("ggplot2")
iq_df <- function(mean, sd = 15) {
  data_frame(
    iq = seq(min(iqs), max(iqs), by = 1),
    density = dnorm(iq, mean, 15),
    mean = mean,
    sd = sd)
}
```

***

```{r, echo = FALSE}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line() +
  geom_point(aes(x = iqs, y = dnorm(iqs, 100, 15)), data = data_frame(iqs))
p
```





Likelihood
========================================================
title: false

Density function `dnorm(xs, mean = 100, sd = 15)` tells us the height of each
value in `xs` when drawn on a normal bell curve

```{r}
# likelihood (density) of each point
dnorm(iqs, 100, 15) %>% round(3)

# likelihood of all points is their product
prod(dnorm(iqs, 100, 15))

# but quantities gets vanishingly small so we use log scale
sum(dnorm(iqs, 100, 15, log = TRUE))
```




Likelihood
========================================================
title: false

Which mean fits the data the best? Below average 85, average 100, or above average 115?

```{r}
sum(dnorm(iqs, 85, 15, log = TRUE))
sum(dnorm(iqs, 100, 15, log = TRUE))
sum(dnorm(iqs, 115, 15, log = TRUE))
```

We just used a "maximum likelihood" criterion to choose among these alternatives!


Likelihood
=============================

- Likelihood is how well data fit in a distribution.
- Classical regression's "line of best fit" estimates the parameters that maximize the likelihood of the data.














